{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFUijIS4H3A6"
      },
      "outputs": [],
      "source": [
        "# Allowed to make changes.\n",
        "!pip install rank_bm25 -q\n",
        "!pip install beir -q\n",
        "!pip install tensorflow_text -q\n",
        "!pip install optimum[intel] -q\n",
        "!pip install wget -q\n",
        "!pip install evaluate -q\n",
        "!python -m pip install optimum[neural-compressor] -q\n",
        "!python -m pip install optimum[openvino,nncf] -q\n",
        "\n",
        "import numpy as np\n",
        "import logging\n",
        "import pathlib, os\n",
        "import torch\n",
        "import pickle\n",
        "import gc\n",
        "import sys\n",
        "import warnings\n",
        "import wget\n",
        "\n",
        "from beir import util, LoggingHandler\n",
        "from beir.retrieval import models\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "from beir.retrieval.evaluation import EvaluateRetrieval\n",
        "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
        "\n",
        "from beir.retrieval import models\n",
        "from beir.retrieval.evaluation import EvaluateRetrieval\n",
        "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
        "import collections\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import timeit\n",
        "from ast import literal_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s88yeOjL12Gv"
      },
      "outputs": [],
      "source": [
        "# Allowed to make changes.\n",
        "\n",
        "# Pre-processing cell. You can use this cell to pre-process input data or load\n",
        "# your models.\n",
        "\n",
        "# loading domain adapted reader model trained in the provided training notebook\n",
        "\n",
        "name = 'devpranjal/deberta-v3-base-domain-adapted'\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "\n",
        "global_model = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
        "# Downloading pargraphs.csv\n",
        "download = wget.download('https://drive.google.com/uc?export=download&id=1GIK2DnacjH5Ddj2QEaCLApvxXayfp9IG')\n",
        "paragraph_ds = pd.read_csv('paragraphs.csv')\n",
        "\n",
        "# Providing corpus in beir format themewise\n",
        "corpus = {}\n",
        "\n",
        "# Dictionary containing all the pargraphs in a theme as a list\n",
        "paragraphs_by_theme = {}\n",
        "\n",
        "# Theme specific bm25 retriever\n",
        "bm25_theme = {}\n",
        "\n",
        "\n",
        "for theme in paragraph_ds['theme'].unique():\n",
        "    paragraphs_by_theme[theme] = paragraph_ds[paragraph_ds['theme']==theme]['paragraph'].tolist()\n",
        "    temp = paragraph_ds[paragraph_ds['theme']==theme]\n",
        "    d = {}\n",
        "    for i in temp.index:\n",
        "        d[temp.loc[i,'id']] = {\n",
        "                'title' : temp.loc[i,'theme'],\n",
        "                'text' : temp.loc[i,'paragraph']\n",
        "            }\n",
        "    corpus[theme] = d\n",
        "\n",
        "    paragraphs = paragraphs_by_theme[theme]\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in paragraphs]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    bm25_theme[theme] = bm25\n",
        "\n",
        "# Dense retriever Model\n",
        "model = DRES(models.SentenceBERT(\"multi-qa-mpnet-base-dot-v1\"), batch_size=16)\n",
        "retriever = EvaluateRetrieval(model, score_function=\"dot\")\n",
        "\n",
        "download = wget.download('https://drive.google.com/uc?export=download&id=1R2OLKRqkgD5TgkCZwuXnCe-jfrhXlTFi')\n",
        "download = wget.download('https://drive.google.com/u/0/uc?id=1aYgNN3u2Vw0q14YcbPuGfYcG0jK-xoKB&export=download')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8RcyC4O2DFk"
      },
      "outputs": [],
      "source": [
        "# Allowed to make changes.\n",
        "def get_theme_model(theme):\n",
        "# Returns global model defined earlier\n",
        "  return global_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Allowed to make changes.\n",
        "def pred_theme_ans(questions, theme_model, pred_out):\n",
        "  theme = questions[0][\"theme\"]\n",
        "  # Theme wise corpus in BEIR format\n",
        "  c = corpus[theme]\n",
        "\n",
        "  # tokenized_corpus = [doc.split(\" \") for doc in paragraphs]\n",
        "  # bm25 = BM25Okapi(tokenized_corpus)\n",
        "  bm25 = bm25_theme[theme]\n",
        "  queries = {}\n",
        "  bm25_results = {}\n",
        "\n",
        "  # Retrieves BM25 of top 5 contexts along with scores for each question \n",
        "  for q in questions:\n",
        "    tokenized_query = q['question'].split(\" \")\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "    l = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i])[-5:]\n",
        "    id = q['id']\n",
        "    queries[id] = q['question']\n",
        "    d = {}\n",
        "    for v in l:\n",
        "      d[str(v)] = doc_scores[v]\n",
        "    bm25_results[id] = d\n",
        "\n",
        "  max_id = {}\n",
        "\n",
        "  # Returns top 5 contexts with combined scores of BM25 and Dense Retriever\n",
        "  results = retriever.retrieve(c, queries, bm25_result = bm25_results, k_values = [5])\n",
        "  for id in results.keys():\n",
        "        # Returns top 1 context\n",
        "        max_id[id] = max(zip(results[id].values(), results[id].keys()))[1] \n",
        "  for q in questions:\n",
        "      id = q['id']\n",
        "      context = c[max_id[id]]['text']\n",
        "\n",
        "      # Passing question and retrived context to our reader model\n",
        "      res = theme_model(question=q['question'], context=context)\n",
        "      paragraph_id = paragraph_ds[paragraph_ds['paragraph']==context]['id'].iloc[0]\n",
        "      ans = {}\n",
        "      ans['question_id'] = id\n",
        "      \n",
        "      # Saving outputs in required format\n",
        "      if res['score'] < 0.01 or res['answer']==\"\":\n",
        "        ans[\"paragraph_id\"] = -1\n",
        "        ans[\"answers\"] = \"\"\n",
        "      else:\n",
        "        ans['paragraph_id'] = paragraph_id\n",
        "        ans['answers'] = res['answer']\n",
        "      pred_out.append(ans)"
      ],
      "metadata": {
        "id": "LGeqS8DPegcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4MGougaIImX"
      },
      "outputs": [],
      "source": [
        "# NOT allowed to make changes. \n",
        "\n",
        "# All theme prediction.\n",
        "questions = json.loads(pd.read_csv(\"sample_input_question_1.csv\").to_json(orient=\"records\"))\n",
        "theme_intervals = json.loads(pd.read_csv(\"sample_theme_interval_1.csv\").to_json(orient=\"records\"))\n",
        "pred_out = []\n",
        "theme_inf_time = {}\n",
        "for theme_interval in theme_intervals:\n",
        "  theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n",
        "  theme = theme_ques[0][\"theme\"]\n",
        "  # Load model fine-tuned for this theme.\n",
        "  theme_model = get_theme_model(theme)\n",
        "  execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, theme_model, pred_out), number=1)\n",
        "  theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n",
        "  print (theme_inf_time)\n",
        "pred_df = pd.DataFrame.from_records(pred_out)\n",
        "pred_df.fillna(value='', inplace=True)\n",
        "# Write prediction to a CSV file. Teams are required to submit this csv file.\n",
        "pred_df.to_csv('sample_output_prediction.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lempoIKsIJ_G"
      },
      "outputs": [],
      "source": [
        "# NOT allowed to make changes. \n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "def calc_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def calc_max_f1(predicted, ground_truths):\n",
        "  max_f1 = 0\n",
        "  if len(ground_truths) == 0:\n",
        "    return len(predicted) == 0\n",
        "  for ground_truth in ground_truths:\n",
        "    f1 = calc_f1(predicted, ground_truth)\n",
        "    max_f1 = max(max_f1, f1)\n",
        "  return max_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HA5KB3RIL4z"
      },
      "outputs": [],
      "source": [
        "# NOT allowed to make changes. \n",
        "\n",
        "# Evaluation methodology.\n",
        "metrics = {}\n",
        "pred = pd.read_csv(\"sample_output_prediction.csv\")\n",
        "pred.fillna(value='', inplace=True)\n",
        "truth = pd.read_csv(\"sample_ground_truth.csv\")\n",
        "truth.fillna(value='', inplace=True)\n",
        "truth.paragraph_id = truth.paragraph_id.apply(literal_eval)\n",
        "truth.answers = truth.answers.apply(literal_eval)\n",
        "questions = pd.read_csv(\"sample_input_question_1.csv\")\n",
        "for idx in pred.index:\n",
        "  q_id = pred[\"question_id\"][idx]\n",
        "  q_rows = questions.loc[questions['id'] == q_id].iloc[-1]\n",
        "  theme = q_rows[\"theme\"]\n",
        "  predicted_paragraph = pred[\"paragraph_id\"][idx]\n",
        "  predicted_ans = pred[\"answers\"][idx]\n",
        "  \n",
        "  if theme not in metrics.keys():\n",
        "    metrics[theme] = {\"true_positive\": 0, \"true_negative\": 0, \"total_predictions\": 0, \"f1_sum\": 0}\n",
        "\n",
        "  truth_row = truth.loc[truth['question_id'] == q_id].iloc[-1]\n",
        "  truth_paragraph_id = [ int(i) for i in truth_row[\"paragraph_id\"] ]\n",
        "  if predicted_paragraph in truth_paragraph_id:\n",
        "    # Increase TP for that theme.\n",
        "    metrics[theme][\"true_positive\"] = metrics[theme][\"true_positive\"] + 1\n",
        "  # -1 prediction in case there is no paragraph which can answer the query.\n",
        "  if predicted_paragraph == -1 and truth_row[\"paragraph_id\"] == []:\n",
        "    # Increase TN.\n",
        "    metrics[theme][\"true_negative\"] = metrics[theme][\"true_negative\"] + 1\n",
        "  # Increase total predictions for that theme.\n",
        "  metrics[theme][\"total_predictions\"] = metrics[theme][\"total_predictions\"] + 1\n",
        "  f1 = calc_max_f1(predicted_ans, truth_row[\"answers\"])\n",
        "  metrics[theme][\"f1_sum\"] = metrics[theme][\"f1_sum\"] + f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tuojd-v8INyd"
      },
      "outputs": [],
      "source": [
        "# NOT allowed to make changes.\n",
        "\n",
        "# Final score.\n",
        "inf_time_threshold = 1000.0 # milliseconds.\n",
        "final_para_score = 0.0\n",
        "final_qa_score = 0.0\n",
        "# Weight would stay hidden from teams.\n",
        "theme_weights = {}\n",
        "for theme in paragraph_ds['theme'].unique():\n",
        "  theme_weights[theme] = 1/len(paragraph_ds['theme'].unique())\n",
        "for theme in metrics:\n",
        "  inf_time_score = 1.0\n",
        "  metric = metrics[theme]\n",
        "  para_score = (metric[\"true_positive\"] + metric[\"true_negative\"]) / metric[\"total_predictions\"] \n",
        "  qa_score = metric[\"f1_sum\"] / metric[\"total_predictions\"]\n",
        "  avg_inf_time = theme_inf_time[theme] / metric[\"total_predictions\"]\n",
        "  if avg_inf_time > inf_time_threshold:\n",
        "    inf_time_score = inf_time_threshold / avg_inf_time\n",
        "  final_qa_score += theme_weights[theme] * inf_time_score * qa_score\n",
        "  final_para_score += theme_weights[theme] * inf_time_score * para_score\n",
        "print (final_para_score)\n",
        "print (final_qa_score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}